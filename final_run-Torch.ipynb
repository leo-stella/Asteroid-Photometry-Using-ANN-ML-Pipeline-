{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "# intel HD does not support CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classical approach to compare with ANN\n",
    "def rolo_b_me(alpha, emission, incidence):\n",
    "    '''\n",
    "    Here we obtain the reflectance for any photometric angle (using degrees as input)  \n",
    "    according to our classic photometric modeling. This model consists of the Sommel-Seeliger \n",
    "    disk function and ROLO as a phase function.\n",
    "\n",
    "    Use example: for phase=30ยบ, emission=0ยบ and incidence=30ยบ - ipwg(30, 0, 30)\n",
    "    \n",
    "    Inputs: phase, emission and incidence angles\n",
    "    Output: reflectance\n",
    "    '''\n",
    "    # parameters\n",
    "    C_0, C_1, A_0, A_1, A_2, A_3, A_4=np.loadtxt('parameters.txt')  \n",
    "    \n",
    "    # converting degrees to radians:\n",
    "    emission=np.deg2rad(emission)    \n",
    "    incidence=np.deg2rad(incidence)\n",
    "    \n",
    "    # computing reflectance according to this photometric model:\n",
    "    disk = (np.cos(incidence)/(np.cos(incidence)+np.cos(emission)))\n",
    "    phase = C_0*np.exp(-C_1*alpha) + A_0 + A_1*alpha + A_2*(alpha)**2 + A_3*(alpha)**3  + A_4*(alpha)**4 \n",
    "    \n",
    "    reflectance = disk * phase\n",
    "    \n",
    "    return reflectance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(ID):\n",
    "    '''\n",
    "    Here we extract features and labels using the ID of each image.\n",
    "    Later we convert these data in the apropiate format for training or validation.\n",
    "    It calls to data_normalization() function.\n",
    "    '''\n",
    "    \n",
    "    iof_n, phase_n, emission_n, incidence_n = data_normalization(ID)\n",
    "    \n",
    "    \n",
    "    # packing data\n",
    "    features = np.zeros((len(phase_n),3)) \n",
    "    features[:,0] = phase_n\n",
    "    features[:,1] = emission_n\n",
    "    features[:,2] = incidence_n\n",
    "    \n",
    "    features = torch.FloatTensor(features)\n",
    "    features = features.to(device)\n",
    "    \n",
    "    labels = np.zeros((len(iof_n),1))\n",
    "    labels[:,0] = iof_n\n",
    "    \n",
    "    labels = torch.FloatTensor(labels)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(ID):\n",
    "    '''\n",
    "    In this function we rescale data between 0 and 1, a very important step before trainning a neural network.\n",
    "    It calls to loading_and_cleaning_data() function.\n",
    "    '''\n",
    "    iof_, phase_, emission_, incidence_ = loading_and_cleaning_data(ID)\n",
    "       \n",
    "    #rescaling data\n",
    "    iof_n=iof_/0.06; phase_n=phase_/90; emission_n=emission_/82; incidence_n=incidence_/82\n",
    "\n",
    "    return iof_n, phase_n, emission_n, incidence_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_and_cleaning_data(ID):\n",
    "    '''\n",
    "    This function is for loading iof, phase, emission, incidence values for each image. \n",
    "    '''\n",
    "    #criterion for removing data:\n",
    "    em, eM = (0, 82)        # emission limits  \n",
    "    im, iM = (0, 82)          # incidence limits\n",
    "    pm, pM = (0, 90)       # phase limits\n",
    "    rm, rM =  (0.001,1)  # reflectance higher limits\n",
    "    \n",
    "    fits_file = fits.getdata(ID, ignore_missing_end=True)\n",
    "    \n",
    "    iof = fits_file[0]\n",
    "    phase = fits_file[1]\n",
    "    emission = fits_file[2]\n",
    "    incidence = fits_file[3]\n",
    "    \n",
    "    idxsort = (emission >= em) & (emission <= eM) & \\\n",
    "              (incidence >= im) & (incidence <= iM) & \\\n",
    "              (phase >= pm) & (phase <= pM) & \\\n",
    "              (iof >= rm) & (iof <= rM) \n",
    "    \n",
    "    return iof[idxsort], phase[idxsort], emission[idxsort], incidence[idxsort] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading ID list of files:\n",
    "files_ID=[]\n",
    "for i in sorted(glob.glob('reduced_phocubes/*reduce.fits')):\n",
    "    files_ID.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# defining global tensors \n",
    "final_training_features = torch.zeros(0)\n",
    "final_training_labels = torch.zeros(0)\n",
    "final_validation_features = torch.zeros(0)\n",
    "final_validation_labels = torch.zeros(0)\n",
    "\n",
    "final_training_features = final_training_features.to(device)\n",
    "final_training_labels = final_training_labels.to(device)\n",
    "final_validation_features = final_validation_features.to(device)\n",
    "final_validation_labels = final_validation_labels.to(device)\n",
    "\n",
    "for file in files_ID:\n",
    "    \n",
    "    # loading training data from files_ID list\n",
    "    _features, _labels = data_preparation(file)\n",
    "    if len(_features) != 0:\n",
    "        np.random.seed(10)\n",
    "        # Selecting 1000 random values of this image\n",
    "        try:\n",
    "            mask_temp = np.random.choice(len(_features),1000,replace=False)\n",
    "        \n",
    "        except:\n",
    "            mask_temp = np.random.choice(len(_features),len(_features),replace=False)\n",
    "        \n",
    "        split = int(len(mask_temp)*(9/10))\n",
    "        mask_train = mask_temp[0:split]\n",
    "        mask_val = mask_temp[split:]\n",
    "        \n",
    "        training_features =  _features[mask_train]\n",
    "        final_training_features = torch.cat([final_training_features,training_features])\n",
    "        \n",
    "        training_labels = _labels[mask_train]\n",
    "        final_training_labels = torch.cat([final_training_labels,training_labels])\n",
    "        \n",
    "        validation_features =  _features[mask_val]\n",
    "        final_validation_features=torch.cat([final_validation_features,validation_features])\n",
    "\n",
    "        validation_labels = _labels[mask_val]\n",
    "        final_validation_labels=torch.cat([final_validation_labels,validation_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417004 46477\n"
     ]
    }
   ],
   "source": [
    "print(len(final_training_features), len(final_validation_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up and defining a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_sizes[0])\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight) \n",
    "        self.fc2 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        self.fc3 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        torch.nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.fc4 = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        torch.nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.fc5 = torch.nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
    "        torch.nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        self.fc6 = torch.nn.Linear(hidden_sizes[4], n_output)\n",
    "        torch.nn.init.kaiming_normal_(self.fc6.weight)\n",
    "        self.relu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid() \n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        x = self.sigmoid(self.fc6(out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_sizes = [15,15,15,15,15]\n",
    "n_output = 1\n",
    "torch.manual_seed(7)\n",
    "model = Net(input_size, hidden_sizes, n_output).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "criterion = torch.nn.MSELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 - L_tr=0.006487206555902958 L_val=0.011827516369521618\n",
      "20 - L_tr=0.0010473834117874503 L_val=0.005182414315640926\n",
      "30 - L_tr=0.0006265772390179336 L_val=0.00434835022315383\n",
      "40 - L_tr=0.0005859573138877749 L_val=0.004151467699557543\n",
      "50 - L_tr=0.0005723173962906003 L_val=0.004071469884365797\n",
      "60 - L_tr=0.0005676352884620428 L_val=0.004025597125291824\n",
      "70 - L_tr=0.0005664670024998486 L_val=0.003994517028331757\n",
      "80 - L_tr=0.0005668990197591484 L_val=0.003971249796450138\n",
      "90 - L_tr=0.0005680486792698503 L_val=0.003952764440327883\n",
      "100 - L_tr=0.0005694955470971763 L_val=0.003937585279345512\n",
      "110 - L_tr=0.0005710669793188572 L_val=0.003924873191863298\n",
      "120 - L_tr=0.000572633056435734 L_val=0.003914106637239456\n",
      "130 - L_tr=0.0005740716005675495 L_val=0.0039049636106938124\n",
      "140 - L_tr=0.0005752513534389436 L_val=0.003897223388776183\n",
      "150 - L_tr=0.0005760762142017484 L_val=0.0038907499983906746\n",
      "160 - L_tr=0.0005765488022007048 L_val=0.003885445650666952\n",
      "170 - L_tr=0.0005767031107097864 L_val=0.0038812439888715744\n",
      "180 - L_tr=0.0005765762180089951 L_val=0.003878073301166296\n",
      "190 - L_tr=0.000576216378249228 L_val=0.0038758534938097\n",
      "200 - L_tr=0.0005756806349381804 L_val=0.0038745012134313583\n"
     ]
    }
   ],
   "source": [
    "epochs=200\n",
    "validation_error=[]\n",
    "batch_extension = 1000\n",
    "n_batches = len(final_training_features) // batch_extension \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for b in range(n_batches):\n",
    "            left = b*batch_extension\n",
    "            right = (b+1) * batch_extension  \n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(final_training_features[left:right,:]) \n",
    "            loss = criterion(prediction, final_training_labels[left:right])\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_val = model(final_validation_features)\n",
    "    Validation_error = criterion(prediction_val, final_validation_labels)\n",
    "    validation_error.append(Validation_error.cpu().numpy())  \n",
    "    \n",
    "    if (epoch % 10 == 0 and epoch != 0):\n",
    "    \n",
    "        print(f'{epoch} - L_tr={loss.item()} L_val={Validation_error.item()}')\n",
    "np.savetxt('validation_error',validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network eficciency using MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction_NEURAL = model(final_validation_features)\n",
    "eff_NEURAL=torch.mean((prediction_NEURAL-final_validation_labels)**2)\n",
    "eff_NEURAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROLO eficciency using MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase=final_validation_features.cpu().numpy()[:,0]*90\n",
    "emission=final_validation_features.cpu().numpy()[:,1]*82\n",
    "incidence=final_validation_features.cpu().numpy()[:,2]*82\n",
    "prediction_ROLO=torch.tensor(rolo_b_me(phase, emission, incidence)/0.06).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0044)\n"
     ]
    }
   ],
   "source": [
    "eff_ROLO=torch.mean((prediction_ROLO-final_validation_labels.t())**2)\n",
    "print(eff_ROLO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9504)\n"
     ]
    }
   ],
   "source": [
    "print((eff_ROLO - eff_NEURAL)*100/eff_ROLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
