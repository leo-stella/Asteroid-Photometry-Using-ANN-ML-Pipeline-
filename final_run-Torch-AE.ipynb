{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "# intel HD does not support CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classical approach to compare with ANN\n",
    "def rolo_b_me(alpha, emission, incidence):\n",
    "    '''\n",
    "    Here we obtain the reflectance for any photometric angle (using degrees as input)  \n",
    "    according to our classic photometric modeling. This model consists of the Sommel-Seeliger \n",
    "    disk function and ROLO as a phase function.\n",
    "\n",
    "    Use example: for phase=30ยบ, emission=0ยบ and incidence=30ยบ - ipwg(30, 0, 30)\n",
    "    \n",
    "    Inputs: phase, emission and incidence angles\n",
    "    Output: reflectance\n",
    "    '''\n",
    "    # parameters\n",
    "    C_0, C_1, A_0, A_1, A_2, A_3, A_4=np.loadtxt('parameters.txt')  \n",
    "    \n",
    "    # converting degrees to radians:\n",
    "    emission=np.deg2rad(emission)    \n",
    "    incidence=np.deg2rad(incidence)\n",
    "    \n",
    "    # computing reflectance according to this photometric model:\n",
    "    disk = (np.cos(incidence)/(np.cos(incidence)+np.cos(emission)))\n",
    "    phase = C_0*np.exp(-C_1*alpha) + A_0 + A_1*alpha + A_2*(alpha)**2 + A_3*(alpha)**3  + A_4*(alpha)**4 \n",
    "    \n",
    "    reflectance = disk * phase\n",
    "    \n",
    "    return reflectance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(ID):\n",
    "    '''\n",
    "     Here we extract features and labels using the ID of each image.\n",
    "    Later we convert these data in the apropiate format for training or validation.\n",
    "    It calls to data_normalization() function.\n",
    "    '''\n",
    "    \n",
    "    iof_n, phase_n, emission_n, incidence_n = data_normalization(ID)\n",
    "    \n",
    "    \n",
    "    # packing data\n",
    "    features = np.zeros((len(phase_n),3)) \n",
    "    features[:,0] = phase_n\n",
    "    features[:,1] = emission_n\n",
    "    features[:,2] = incidence_n\n",
    "    \n",
    "    features = torch.FloatTensor(features)\n",
    "    features = features.to(device)\n",
    "    \n",
    "    labels = np.zeros((len(iof_n),1))\n",
    "    labels[:,0] = iof_n\n",
    "    \n",
    "    labels = torch.FloatTensor(labels)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(ID):\n",
    "    '''\n",
    "    In this function we rescale data between 0 and 1, a very important step before trainning a neural network.\n",
    "    It calls to loading_and_cleaning_data() function.\n",
    "    '''\n",
    "    iof_, phase_, emission_, incidence_ = loading_and_cleaning_data(ID)\n",
    "       \n",
    "    #normalizing data\n",
    "    iof_n=iof_/0.06; phase_n=phase_/90; emission_n=emission_/82; incidence_n=incidence_/82\n",
    "\n",
    "    return iof_n, phase_n, emission_n, incidence_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_and_cleaning_data(ID):\n",
    "    '''\n",
    "    This function is for loading iof, phase, emission, incidence values for each image. \n",
    "    '''\n",
    "    #criterion for removing data:\n",
    "    em, eM = (0, 82)        # emission limits  \n",
    "    im, iM = (0, 82)          # incidence limits\n",
    "    pm, pM = (0, 90)       # phase limits\n",
    "    rm, rM =  (0.001,1)  # reflectance higher limits\n",
    "    \n",
    "    fits_file = fits.getdata(ID, ignore_missing_end=True)\n",
    "    \n",
    "    iof = fits_file[0]\n",
    "    phase = fits_file[1]\n",
    "    emission = fits_file[2]\n",
    "    incidence = fits_file[3]\n",
    "    \n",
    "    idxsort = (emission >= em) & (emission <= eM) & \\\n",
    "              (incidence >= im) & (incidence <= iM) & \\\n",
    "              (phase >= pm) & (phase <= pM) & \\\n",
    "              (iof >= rm) & (iof <= rM) \n",
    "    \n",
    "    return iof[idxsort], phase[idxsort], emission[idxsort], incidence[idxsort] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading ID list of files:\n",
    "files_ID=[]\n",
    "for i in sorted(glob.glob('reduced_phocubes/*reduce.fits')):\n",
    "    files_ID.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# defining global tensors \n",
    "\n",
    "final_training_features = torch.zeros(0)\n",
    "final_training_labels = torch.zeros(0)\n",
    "final_validation_features = torch.zeros(0)\n",
    "final_validation_labels = torch.zeros(0)\n",
    "\n",
    "final_training_features = final_training_features.to(device)\n",
    "final_training_labels = final_training_labels.to(device)\n",
    "final_validation_features = final_validation_features.to(device)\n",
    "final_validation_labels = final_validation_labels.to(device)\n",
    "\n",
    "for file in files_ID:\n",
    "    \n",
    "    # loading training data from files_ID list\n",
    "    _features, _labels = data_preparation(file)\n",
    "    if len(_features) != 0:\n",
    "        np.random.seed(10)\n",
    "        # Selecting 1000 random values of this image\n",
    "        try:\n",
    "            mask_temp = np.random.choice(len(_features),1000,replace=False)\n",
    "        \n",
    "        except:\n",
    "            mask_temp = np.random.choice(len(_features),len(_features),replace=False)\n",
    "        \n",
    "        split = int(len(mask_temp)*(9/10))\n",
    "        mask_train = mask_temp[0:split]\n",
    "        mask_val = mask_temp[split:]\n",
    "        \n",
    "        training_features =  _features[mask_train]\n",
    "        final_training_features = torch.cat([final_training_features,training_features])\n",
    "        \n",
    "        training_labels = _labels[mask_train]\n",
    "        final_training_labels = torch.cat([final_training_labels,training_labels])\n",
    "\n",
    "        validation_features =  _features[mask_val]\n",
    "        final_validation_features=torch.cat([final_validation_features,validation_features])\n",
    "\n",
    "        validation_labels = _labels[mask_val]\n",
    "        final_validation_labels=torch.cat([final_validation_labels,validation_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROLO eficciency using MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0044)\n"
     ]
    }
   ],
   "source": [
    "#ROLO eficciency using MSE:\n",
    "phase=final_validation_features.cpu().numpy()[:,0]*90\n",
    "emission=final_validation_features.cpu().numpy()[:,1]*82\n",
    "incidence=final_validation_features.cpu().numpy()[:,2]*82\n",
    "prediction_ROLO=torch.tensor(rolo_b_me(phase, emission, incidence)/0.06).to(device)\n",
    "eff_ROLO=torch.mean((prediction_ROLO-final_validation_labels.t())**2)\n",
    "print(eff_ROLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417025 46480\n"
     ]
    }
   ],
   "source": [
    "print(len(final_training_features), len(final_validation_features))\n",
    "final_training_features = final_training_features.numpy()\n",
    "final_validation_features = final_validation_features.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 0.0088 - accuracy: 0.8435 - val_loss: 2.6698e-04 - val_accuracy: 0.9718\n",
      "Epoch 2/20\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 1.5683e-04 - accuracy: 0.9758 - val_loss: 1.0369e-04 - val_accuracy: 0.9826\n",
      "Epoch 3/20\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 6.4331e-05 - accuracy: 0.9816 - val_loss: 5.0152e-05 - val_accuracy: 0.9750\n",
      "Epoch 4/20\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 4.3965e-05 - accuracy: 0.9809 - val_loss: 3.7072e-05 - val_accuracy: 0.9818\n",
      "Epoch 5/20\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 3.5506e-05 - accuracy: 0.9818 - val_loss: 3.0966e-05 - val_accuracy: 0.9823\n",
      "Epoch 6/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 3.0876e-05 - accuracy: 0.9813 - val_loss: 4.5476e-05 - val_accuracy: 0.9540\n",
      "Epoch 7/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 2.7086e-05 - accuracy: 0.9820 - val_loss: 2.4795e-05 - val_accuracy: 0.9721\n",
      "Epoch 8/20\n",
      "418/418 [==============================] - 2s 6ms/step - loss: 2.4156e-05 - accuracy: 0.9819 - val_loss: 3.2460e-05 - val_accuracy: 0.9644\n",
      "Epoch 9/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 2.2800e-05 - accuracy: 0.9823 - val_loss: 2.8925e-05 - val_accuracy: 0.9833\n",
      "Epoch 10/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 2.1233e-05 - accuracy: 0.9817 - val_loss: 1.8058e-05 - val_accuracy: 0.9866\n",
      "Epoch 11/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 2.0277e-05 - accuracy: 0.9826 - val_loss: 1.9904e-05 - val_accuracy: 0.9861\n",
      "Epoch 12/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 1.9539e-05 - accuracy: 0.9827 - val_loss: 1.3442e-05 - val_accuracy: 0.9872\n",
      "Epoch 13/20\n",
      "418/418 [==============================] - 2s 6ms/step - loss: 1.7836e-05 - accuracy: 0.9836 - val_loss: 1.4892e-05 - val_accuracy: 0.9869\n",
      "Epoch 14/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 1.7560e-05 - accuracy: 0.9833 - val_loss: 2.2834e-05 - val_accuracy: 0.9838\n",
      "Epoch 15/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 1.6678e-05 - accuracy: 0.9832 - val_loss: 1.4526e-05 - val_accuracy: 0.9861\n",
      "Epoch 16/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 1.7107e-05 - accuracy: 0.9836 - val_loss: 1.9014e-05 - val_accuracy: 0.9810\n",
      "Epoch 17/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 1.5739e-05 - accuracy: 0.9837 - val_loss: 1.2323e-05 - val_accuracy: 0.9874\n",
      "Epoch 18/20\n",
      "418/418 [==============================] - 2s 5ms/step - loss: 1.5512e-05 - accuracy: 0.9836 - val_loss: 2.6741e-05 - val_accuracy: 0.9656\n",
      "Epoch 19/20\n",
      "418/418 [==============================] - 2s 6ms/step - loss: 1.5044e-05 - accuracy: 0.9841 - val_loss: 2.5783e-05 - val_accuracy: 0.9847\n",
      "Epoch 20/20\n",
      "418/418 [==============================] - 2s 6ms/step - loss: 1.5240e-05 - accuracy: 0.9837 - val_loss: 1.9815e-05 - val_accuracy: 0.9803\n",
      "Model: \"autoencoder_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_22 (Sequential)  (None, 9)                 5001      \n",
      "                                                                 \n",
      " sequential_23 (Sequential)  (None, 3)                 4995      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,996\n",
      "Trainable params: 9,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score ,recall_score\n",
    "from tensorflow.keras import layers,losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self,latent_dim=9):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.latent_dim=9\n",
    "        self.encoder=tf.keras.Sequential([\n",
    "            layers.Dense(64,activation=\"relu\"),\n",
    "            layers.Dense(64,activation=\"relu\"),\n",
    "            layers.Dense(latent_dim,activation=\"relu\")])\n",
    "        \n",
    "        self.decoder=tf.keras.Sequential([\n",
    "            layers.Dense(64,activation=\"relu\"),\n",
    "            layers.Dense(64,activation=\"relu\"),\n",
    "            layers.Dense(3,activation=\"sigmoid\")])\n",
    "        \n",
    "    def call(self,x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded=self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "autoencoder=Autoencoder()\n",
    "autoencoder.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "autoencoder.fit(final_training_features,final_training_features, \n",
    "                validation_data=(final_validation_features, final_validation_features), epochs=20, batch_size=1000)\n",
    "\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417025, 9)\n",
      "(46480, 9)\n",
      "(417025, 12)\n",
      "(46480, 12)\n"
     ]
    }
   ],
   "source": [
    "x_training_encoded = autoencoder.encoder(final_training_features).numpy()\n",
    "x_vaidation_encoded = autoencoder.encoder(final_validation_features).numpy()\n",
    "\n",
    "print(x_training_encoded.shape)\n",
    "print(x_vaidation_encoded.shape)\n",
    "\n",
    "import numpy as np\n",
    "x_training_concat = np.concatenate((final_training_features, x_training_encoded), axis = 1)\n",
    "x_validation_concat = np.concatenate((final_validation_features, x_vaidation_encoded), axis = 1)\n",
    "\n",
    "print(x_training_concat.shape)\n",
    "print(x_validation_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def correlated_features(data,threshold):\n",
    "    corr_col=set()\n",
    "    confusion=data.corr()\n",
    "    for i in range(confusion.shape[0]):\n",
    "        for j in range(i): \n",
    "             if confusion.iloc[i,j]>threshold:\n",
    "                    corr_col.add(confusion.columns[i])\n",
    "    return corr_col\n",
    "\n",
    "pd_x_training_concat = pd.DataFrame (x_training_concat)\n",
    "pd_x_validation_concat = pd.DataFrame (x_validation_concat)\n",
    "\n",
    "corr_features= correlated_features(pd_x_training_concat , 0.9)\n",
    "print(len(corr_features))\n",
    "\n",
    "x_training_corr = pd_x_training_concat.drop(corr_features,axis=1)\n",
    "x_validation_corr = pd_x_validation_concat.drop(corr_features,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "pca = PCA()\n",
    "pca.fit(x_training_corr)\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')  \n",
    "plt.title('Segmentation Dataset Explained Variance')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417025, 3)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "x_training_pca = pca.fit_transform(x_training_corr)\n",
    "x_validation_pca = pca.fit_transform(x_validation_corr)\n",
    "print(x_training_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "final_training_features = torch.from_numpy(x_training_pca)\n",
    "final_validation_features = torch.from_numpy(x_validation_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up and defining a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_sizes[0])\n",
    "        torch.nn.init.kaiming_normal_(self.fc1.weight) \n",
    "        self.fc2 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        torch.nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        self.fc3 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        torch.nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.fc4 = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        torch.nn.init.kaiming_normal_(self.fc4.weight)\n",
    "        self.fc5 = torch.nn.Linear(hidden_sizes[3], hidden_sizes[4])\n",
    "        torch.nn.init.kaiming_normal_(self.fc5.weight)\n",
    "        self.fc6 = torch.nn.Linear(hidden_sizes[4], n_output)\n",
    "        torch.nn.init.kaiming_normal_(self.fc6.weight)\n",
    "        self.relu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid() \n",
    "        \n",
    "  \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        x = self.sigmoid(self.fc6(out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_sizes = [15,15,15,15,15]\n",
    "n_output = 1\n",
    "torch.manual_seed(7)\n",
    "model = Net(input_size, hidden_sizes, n_output).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 - L_tr=0.06345343589782715 L_val=0.023527109995484352\n",
      "20 - L_tr=0.006245095748454332 L_val=0.006716305855661631\n",
      "30 - L_tr=0.0019489217083901167 L_val=0.005392648279666901\n",
      "40 - L_tr=0.001204721280373633 L_val=0.0048143756575882435\n",
      "50 - L_tr=0.0009424234158359468 L_val=0.004489203914999962\n",
      "60 - L_tr=0.0008020081440918148 L_val=0.004296823870390654\n",
      "70 - L_tr=0.0007185242138803005 L_val=0.004168986808508635\n",
      "80 - L_tr=0.000667149608489126 L_val=0.00407362449914217\n",
      "90 - L_tr=0.0006338558159768581 L_val=0.00399733055382967\n",
      "100 - L_tr=0.0006103908526711166 L_val=0.00393557408824563\n",
      "110 - L_tr=0.0005926621961407363 L_val=0.0038921867962926626\n",
      "120 - L_tr=0.0005793801974505186 L_val=0.003869822481647134\n",
      "130 - L_tr=0.0005701353074982762 L_val=0.0038610976189374924\n",
      "140 - L_tr=0.0005641361349262297 L_val=0.003858401905745268\n",
      "150 - L_tr=0.0005602178862318397 L_val=0.0038577914237976074\n",
      "160 - L_tr=0.000557492021471262 L_val=0.003857832867652178\n",
      "170 - L_tr=0.0005554730305448174 L_val=0.003857766278088093\n",
      "180 - L_tr=0.0005539265112020075 L_val=0.003857085481286049\n",
      "190 - L_tr=0.000552723475266248 L_val=0.0038556826766580343\n",
      "200 - L_tr=0.0005517855752259493 L_val=0.003853745758533478\n"
     ]
    }
   ],
   "source": [
    "epochs=201\n",
    "validation_error=[]\n",
    "batch_extension = 1000\n",
    "n_batches = len(final_training_features) // batch_extension \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for b in range(n_batches):\n",
    "            left = b*batch_extension\n",
    "            right = (b+1) * batch_extension  \n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(final_training_features[left:right,:]) \n",
    "            loss = criterion(prediction, final_training_labels[left:right])\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "            \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_val = model(final_validation_features)\n",
    "    Validation_error = criterion(prediction_val, final_validation_labels)\n",
    "    validation_error.append(Validation_error.cpu().numpy())  \n",
    "    \n",
    "    if (epoch % 10 == 0 and epoch != 0):\n",
    "    \n",
    "        print(f'{epoch} - L_tr={loss.item()} L_val={Validation_error.item()}')\n",
    "np.savetxt('validation_error',validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network eficciency using MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction_NEURAL = model(final_validation_features)\n",
    "eff_NEURAL=torch.mean((prediction_NEURAL-final_validation_labels)**2)\n",
    "eff_NEURAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvement percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.4385)\n"
     ]
    }
   ],
   "source": [
    "print((eff_ROLO - eff_NEURAL)*100/eff_ROLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
